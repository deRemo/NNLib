{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "VERSIONE 7\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "    \n",
    "    #Adding input layer(first hidden layer): you must specify the input_size\n",
    "    #Adding hidden/output layer: if the layer is dense, the input_size is inferred from the previous layer\n",
    "    def add_layer(self, l, num_units, f, input_size=None):\n",
    "        if(l == \"dense\" or l == \"Dense\" or l == \"d\" or l == \"D\"):\n",
    "            if(input_size == None):\n",
    "                input_size = self.layers[-1].num_units\n",
    "            self.layers.append(Layer(num_units, activations(f), input_size))\n",
    "        \n",
    "    def feed_forward(self, batch):\n",
    "        current_input = batch\n",
    "        for l in self.layers:\n",
    "            l.input = current_input\n",
    "            l.net = np.dot(l.input, l.weights) + l.bias # dot(in,w)+bias\n",
    "            l.out = l.f(l.net)\n",
    "            \n",
    "            current_input = l.out\n",
    "        return current_input\n",
    "    \n",
    "    def back_propagation(self, FF_output, targets, lr):\n",
    "        err_signal=0\n",
    "        bp=0\n",
    "            \n",
    "        self.layers.reverse()\n",
    "        for index,l in enumerate(self.layers):\n",
    "            #weight update & backprop sum\n",
    "            if(index==0): #OUTPUT LAYER\n",
    "                err = targets - FF_output\n",
    "                err_signal = err * l.d_f(l.net)\n",
    "                l.d_weights = np.dot(l.input.T, err_signal)\n",
    "                \n",
    "                bp = err_signal * l.weights.T\n",
    "            else: #HIDDEN LAYERS\n",
    "                err_signal = err * l.d_f(l.net)\n",
    "                l.d_weights = np.dot(l.input.T, err_signal)\n",
    "                \n",
    "                bp = err_signal\n",
    "            l.weights += lr * l.d_weights\n",
    "            \n",
    "            #bias update\n",
    "            l.bias += np.sum(lr*err_signal, axis=0)\n",
    "        self.layers.reverse()\n",
    "        \n",
    "        #compute loss and accuracy\n",
    "        loss = np.sum(err, axis=0)\n",
    "        accuracy = self.get_accuracy(err)\n",
    "        \n",
    "        return loss, accuracy\n",
    "    \n",
    "    def train(self,batch, targets, lr):\n",
    "        return self.back_propagation(self.feed_forward(batch), targets, lr)\n",
    "        \n",
    "    def fit(self, dataset, targets, lr, batch_size=dataset.size, train_percentage=1.0, epochs=1, patience=5): \n",
    "        timer = Timer()\n",
    "        timer.start()\n",
    "        \n",
    "        if train_percentage < 0.0 or train_percentage > 1.0:\n",
    "            print(\"ERROR, train_percentage is a percentage\")\n",
    "            return -1\n",
    "        train_set, valid_set = self.dataset_ratio_split(dataset, train_percentage)\n",
    "        train_targets, valid_targets = self.dataset_ratio_split(targets.T, train_percentage)\n",
    "        \n",
    "        err = 0\n",
    "        best_accuracy = float(\"-inf\")\n",
    "        best_loss = 0\n",
    "        best_model = 0\n",
    "        arr_loss = []\n",
    "        arr_accuracy = []\n",
    "        for i in range(0, epochs):\n",
    "            \n",
    "            tr_loss, tr_accuracy = self.train(train_set, train_targets, lr)\n",
    "            arr_loss.append(tr_loss)\n",
    "            arr_accuracy.append(tr_accuracy)\n",
    "            \n",
    "            if valid_set.size != 0 and valid_targets.size !=0:\n",
    "                vl_loss, vl_accuracy = self.evaluate(valid_set, valid_targets)\n",
    "                \n",
    "                if(vl_accuracy > best_accuracy):\n",
    "                    best_accuracy = vl_accuracy\n",
    "                    best_loss = vl_loss\n",
    "                    best_model = copy.deepcopy(self)\n",
    "                else:\n",
    "                    if(patience != 0):\n",
    "                        patience = patience - 1\n",
    "                    else:\n",
    "                        self = best_model\n",
    "                        print(\"break at epoch \", i)\n",
    "                        break;\n",
    "            \n",
    "            if i == epochs-1:\n",
    "                print(\"tr - loss \", tr_loss, \" accuracy \", tr_accuracy)\n",
    "                print(\"vl - loss \", best_loss, \" accuracy \", best_accuracy)\n",
    "            #print(\"epoch \", i, \"\\nerr_tot \\n\", loss)\n",
    "        plt.figure()\n",
    "        plt.subplot(221)\n",
    "        plt.plot(range(i+1), arr_loss, 'r-', label=\"loss\")\n",
    "        plt.title(\"loss\")\n",
    "        #plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "        \n",
    "        plt.subplot(2,2,2)\n",
    "        plt.plot(range(i+1), arr_accuracy, 'b-', label=\"accuracy\")\n",
    "        plt.title(\"accuracy\")\n",
    "        #plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "        #print(\"epoch \", i, \"\\nerr_tot \\n\", tr_loss)\n",
    "        timer.stop()\n",
    "        print(\"elapsed: \\n\", timer.get(), \" ms\")\n",
    "    \n",
    "    def evaluate(self, dataset, targets):\n",
    "        out = self.feed_forward(dataset)\n",
    "        err = targets - out\n",
    "        loss = np.sum(err, axis=0)\n",
    "        accuracy = self.get_accuracy(err)\n",
    "        \n",
    "        return loss, accuracy\n",
    "        \n",
    "        \n",
    "    def predict(self, dataset):\n",
    "        out = self.feed_forward(dataset)\n",
    "        return out\n",
    "            \n",
    "    def dataset_ratio_split(self, dataset, train_percentage):\n",
    "        rows = dataset.shape[0]\n",
    "        train_size = np.int(np.around(rows * train_percentage))\n",
    "        \n",
    "        return np.array_split(dataset, [train_size])\n",
    "    \n",
    "    def dataset_batch_split(self, dataset, batch_size):\n",
    "        rows = dataset.shape[0]\n",
    "        \n",
    "        return np.array_split(a, rows/batch_size)\n",
    "    \n",
    "    def get_accuracy(self, err):\n",
    "        rounded = np.around(err)\n",
    "        accuracy = (rounded.size - np.count_nonzero(rounded))/rounded.size\n",
    "        \n",
    "        return accuracy\n",
    "        \n",
    "    def print_weights(self, label=\"w\"):\n",
    "        for i in range(0, len(nn.layers)):\n",
    "            print(label, \": layer \",i)\n",
    "            print(nn.layers[i].weights)\n",
    "            print(nn.layers[i].bias)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "VERSIONE 6\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "    \n",
    "    #Adding input layer(first hidden layer): you must specify the input_size\n",
    "    #Adding hidden/output layer: if the layer is dense, the input_size is inferred from the previous layer\n",
    "    def add_layer(self, l, num_units, f, input_size=None):\n",
    "        if(l == \"dense\" or l == \"Dense\" or l == \"d\" or l == \"D\"):\n",
    "            if(input_size == None):\n",
    "                input_size = self.layers[-1].num_units\n",
    "            self.layers.append(Layer(num_units, activations(f), input_size))\n",
    "        \n",
    "    def feed_forward(self, batch):\n",
    "        current_input = batch\n",
    "        for l in self.layers:\n",
    "            l.input = current_input\n",
    "            l.net = np.dot(l.input, l.weights) + l.bias # dot(in,w)+bias\n",
    "            l.out = l.f(l.net)\n",
    "            \n",
    "            current_input = l.out\n",
    "        return current_input\n",
    "    \n",
    "    def back_propagation(self, FF_output, targets, lr):\n",
    "        bp=0\n",
    "        err_signal=0\n",
    "        \n",
    "        self.layers.reverse()\n",
    "        for index,l in enumerate(self.layers):\n",
    "            #weight update\n",
    "            if(index==0): #OUTPUT LAYER\n",
    "                err = targets.T - FF_output\n",
    "                err_signal = err * l.d_f(l.net)\n",
    "                l.d_weights = lr * np.dot(l.f(l.input).T, err_signal)\n",
    "            else: #HIDDEN LAYERS\n",
    "                err_signal = np.multiply(l.d_f(l.net), bp.T)\n",
    "                print(\"es\\n\",err_signal)\n",
    "                l.d_weights = lr * np.dot(l.f(l.input).T, err_signal)\n",
    "            l.weights += l.d_weights\n",
    "            \n",
    "            #bias update\n",
    "            l.bias += np.sum(lr*err_signal, axis=0)\n",
    "            \n",
    "            #backprop sum\n",
    "            if(index != len(self.layers)-1):\n",
    "                flat_err_signal = np.prod(err_signal, axis=0)\n",
    "                \n",
    "                if(index==0):\n",
    "                    bp += np.multiply(l.weights, flat_err_signal)\n",
    "                else:\n",
    "                    bp = np.add(np.multiply(l.weights, flat_err_signal), bp)\n",
    "                print(\"bp\\n\", bp)\n",
    "        self.layers.reverse()\n",
    "            \n",
    "        return err\n",
    "    \n",
    "    def train(self,batch, targets, lr):\n",
    "        return self.back_propagation(self.feed_forward(batch), targets, lr)\n",
    "        \n",
    "    def fit(self, dataset, targets, lr, max_it=10000): \n",
    "        for i in range(0, len(nn.layers)):\n",
    "            print(\"old_w: layer \",i)\n",
    "            print(nn.layers[i].weights)\n",
    "            print(nn.layers[i].bias)\n",
    "        \n",
    "        err=0\n",
    "        for i in range(0,max_it):\n",
    "            err=self.train(dataset, targets, lr)\n",
    "            \n",
    "        for i in range(0, len(nn.layers)):\n",
    "            print(\"new_w: layer \",i)\n",
    "            print(nn.layers[i].weights)\n",
    "            print(nn.layers[i].bias)\n",
    "        print(\"err: \\n\", err)\n",
    "    \n",
    "    def predict(self, values):\n",
    "        print(\"prediction: \")\n",
    "        print(self.feed_forward(values))\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "VERSIONE 5\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "    \n",
    "    #Adding input layer(first hidden layer): you must specify the input_size\n",
    "    #Adding hidden/output layer: if the layer is dense, the input_size is inferred from the previous layer\n",
    "    def add_layer(self, l, num_units, f, input_size=None):\n",
    "        if(l == \"dense\" or l == \"Dense\" or l == \"d\" or l == \"D\"):\n",
    "            if(input_size == None):\n",
    "                input_size = self.layers[-1].num_units\n",
    "            \n",
    "            self.layers.append(Layer(num_units, activations(f), input_size))\n",
    "        \n",
    "    def feed_forward(self, batch):\n",
    "        current_input = batch\n",
    "        for l in self.layers:\n",
    "            l.input = current_input\n",
    "            l.net = np.dot(l.input, l.weights) + l.bias # dot(in,w)+bias\n",
    "            l.out = l.f(l.net)\n",
    "            \n",
    "            current_input = l.out\n",
    "        return current_input\n",
    "    \n",
    "    def back_propagation(self, FF_output, targets, lr):\n",
    "        bp=0\n",
    "        err_signal=0\n",
    "        \n",
    "        self.layers.reverse()\n",
    "        for index,l in enumerate(self.layers):\n",
    "            #weight update\n",
    "            if(index==0): #OUTPUT LAYER\n",
    "                err = targets.T - FF_output\n",
    "                err_signal = err * l.d_f(l.net)\n",
    "                l.d_weights = lr * np.dot(l.f(l.input).T, err_signal) \n",
    "            else:\n",
    "                err_signal = np.multiply(l.d_f(l.net), bp.T)\n",
    "                l.d_weights = lr * err_signal * l.f(l.input)\n",
    "            l.weights += l.d_weights\n",
    "            \n",
    "            #bias update\n",
    "            l.bias += np.sum(lr*err_signal, axis=0)\n",
    "            \n",
    "            #backprop sum\n",
    "            flat_err_signal = np.prod(err_signal, axis=0)\n",
    "            bp += flat_err_signal * l.weights\n",
    "        self.layers.reverse()\n",
    "            \n",
    "        return err\n",
    "    \n",
    "    def train(self,batch, targets, lr):\n",
    "        return self.back_propagation(self.feed_forward(batch), targets, lr)\n",
    "        \n",
    "    def fit(self, dataset, targets, lr, max_it=10000): \n",
    "        for i in range(0, len(nn.layers)):\n",
    "            print(\"old_w: layer \",i)\n",
    "            print(nn.layers[i].weights)\n",
    "            print(nn.layers[i].bias)\n",
    "        \n",
    "        err=0\n",
    "        for i in range(0,max_it):\n",
    "            err=self.train(dataset, targets, lr)\n",
    "            \n",
    "        for i in range(0, len(nn.layers)):\n",
    "            print(\"new_w: layer \",i)\n",
    "            print(nn.layers[i].weights)\n",
    "            print(nn.layers[i].bias)\n",
    "        print(\"err: \\n\", err)\n",
    "    \n",
    "    def predict(self, values):\n",
    "        print(\"prediction: \")\n",
    "        print(self.feed_forward(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "VERSIONE 4\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.layers.append(Layer(1, activations('sigmoid'))) #SEMPRE SIGMOID PER L'OUTPUT LAYER(??)\n",
    "    \n",
    "    def add_layer(self,l, num_units, f):\n",
    "        if(l == \"dense\" or l == \"Dense\" or l == \"d\" or l == \"D\"):\n",
    "            self.layers.append(Layer(num_units, activations(f)))\n",
    "        \n",
    "    def feed_forward(self, batch):\n",
    "        current_input = batch\n",
    "        \n",
    "        self.layers.reverse()\n",
    "        for l in self.layers:\n",
    "            l.input = current_input\n",
    "            l.net = np.dot(l.input, l.weights) + l.bias\n",
    "            l.out = l.f(l.net)\n",
    "            \n",
    "            current_input = l.out\n",
    "        self.layers.reverse()\n",
    "        return current_input\n",
    "    \n",
    "    def back_propagation(self, FF_output, targets, lr):\n",
    "        bp=0\n",
    "        \n",
    "        for index,l in enumerate(self.layers):\n",
    "            if(index==0): #OUTPUT LAYER\n",
    "                err = targets.T - FF_output\n",
    "                l.d_weights = lr * np.dot(l.f(l.input).T, err*l.d_f(l.net)) #l.input=batch in this case\n",
    "            else:\n",
    "                l.d_weights = lr * l.d_f(l.net) * bp * l.f(l.input)\n",
    "            l.weights += l.d_weights\n",
    "            bp += l.d_weights * l.weights\n",
    "            \n",
    "        return err\n",
    "    \n",
    "    def train(self,batch, targets, lr):\n",
    "        return self.back_propagation(self.feed_forward(batch), targets, lr)\n",
    "    \n",
    "    def fit(self, dataset, targets, lr, max_it=10000):\n",
    "        #inizializzo i pesi; il numero dipende dal dataset \n",
    "        columns = np.size(dataset,1)\n",
    "        \n",
    "        self.layers.reverse()\n",
    "        prev_output=0\n",
    "        for index,l in enumerate(self.layers):\n",
    "            if(index==0):#PRIMO HIDDEN LAYER, QUELLO CHE PRENDE IN INPUT IL DATASET\n",
    "                l.init_weights(columns)\n",
    "            else:\n",
    "                l.init_weights(prev_output)\n",
    "            prev_output=l.num_units\n",
    "        self.layers.reverse()  \n",
    "        \n",
    "        for i in range(0, len(nn.layers)):\n",
    "            print(\"old_w: layer \",i)\n",
    "            print(nn.layers[i].weights)\n",
    "        \n",
    "        err=0\n",
    "        #for i in range(0,max_it):\n",
    "        #    err=self.train(dataset, targets, lr)\n",
    "            \n",
    "        for i in range(0, len(nn.layers)):\n",
    "            print(\"new_w: layer \",i)\n",
    "            print(nn.layers[i].weights)\n",
    "            \n",
    "        print(\"err: \")\n",
    "        print(err)\n",
    "    \n",
    "    def predict(self, value):\n",
    "        print(\"prediction: \")\n",
    "        print(self.feed_forward(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "VERSIONE 4\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.layers.append(Layer(1, activations('sigmoid'))) #SEMPRE SIGMOID PER L'OUTPUT LAYER(??)\n",
    "    \n",
    "    def add_layer(self,l, num_units, f):\n",
    "        if(l == \"dense\" or l == \"Dense\" or l == \"d\" or l == \"D\"):\n",
    "            self.layers.append(Layer(num_units, activations(f)))\n",
    "        \n",
    "    def feed_forward(self, batch):\n",
    "        current_input = batch\n",
    "        \n",
    "        self.layers.reverse()\n",
    "        for l in self.layers:\n",
    "            l.input = current_input\n",
    "            l.net = np.dot(l.input, l.weights) + l.bias\n",
    "            l.out = l.f(l.net)\n",
    "            \n",
    "            current_input = l.out\n",
    "        self.layers.reverse()\n",
    "        return current_input\n",
    "    \n",
    "    def back_propagation(self, FF_output, targets, lr):\n",
    "        bp=0\n",
    "        \n",
    "        for index,l in enumerate(self.layers):\n",
    "            if(index==0): #OUTPUT LAYER\n",
    "                err = targets.T - FF_output\n",
    "                l.d_weights = lr * np.dot(l.f(l.input).T, err*l.d_f(l.net)) #l.input=batch in this case\n",
    "            else:\n",
    "                l.d_weights = lr * l.d_f(l.net) * bp * l.f(l.input)\n",
    "            l.weights += l.d_weights\n",
    "            bp += l.d_weights * l.weights\n",
    "            \n",
    "        return err\n",
    "    \n",
    "    def train(self,batch, targets, lr):\n",
    "        return self.back_propagation(self.feed_forward(batch), targets, lr)\n",
    "    \n",
    "    def fit(self, dataset, targets, lr, max_it=10000):\n",
    "        #inizializzo i pesi; il numero dipende dal dataset \n",
    "        columns = np.size(dataset,1)\n",
    "        \n",
    "        self.layers.reverse()\n",
    "        prev_output=0\n",
    "        for index,l in enumerate(self.layers):\n",
    "            if(index==0):#PRIMO HIDDEN LAYER, QUELLO CHE PRENDE IN INPUT IL DATASET\n",
    "                l.init_weights(columns)\n",
    "            else:\n",
    "                l.init_weights(prev_output)\n",
    "            prev_output=l.num_units\n",
    "        self.layers.reverse()  \n",
    "        \n",
    "        for i in range(0, len(nn.layers)):\n",
    "            print(\"old_w: layer \",i)\n",
    "            print(nn.layers[i].weights)\n",
    "        \n",
    "        err=0\n",
    "        #for i in range(0,max_it):\n",
    "        #    err=self.train(dataset, targets, lr)\n",
    "            \n",
    "        for i in range(0, len(nn.layers)):\n",
    "            print(\"new_w: layer \",i)\n",
    "            print(nn.layers[i].weights)\n",
    "            \n",
    "        print(\"err: \")\n",
    "        print(err)\n",
    "    \n",
    "    def predict(self, value):\n",
    "        print(\"prediction: \")\n",
    "        print(self.feed_forward(value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "VERSIONE 6\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "    \n",
    "    #Adding input layer(first hidden layer): you must specify the input_size\n",
    "    #Adding hidden/output layer: if the layer is dense, the input_size is inferred from the previous layer\n",
    "    def add_layer(self, l, num_units, f, input_size=None):\n",
    "        if(l == \"dense\" or l == \"Dense\" or l == \"d\" or l == \"D\"):\n",
    "            if(input_size == None):\n",
    "                input_size = self.layers[-1].num_units\n",
    "            self.layers.append(Layer(num_units, activations(f), input_size))\n",
    "        \n",
    "    def feed_forward(self, batch):\n",
    "        current_input = batch\n",
    "        for l in self.layers:\n",
    "            l.input = current_input\n",
    "            l.net = np.dot(l.input, l.weights) + l.bias # dot(in,w)+bias\n",
    "            l.out = l.f(l.net)\n",
    "            \n",
    "            current_input = l.out\n",
    "        return current_input\n",
    "    \n",
    "    def back_propagation(self, FF_output, targets, lr):\n",
    "        bp=0\n",
    "        err_signal=0\n",
    "        \n",
    "        self.layers.reverse()\n",
    "        for index,l in enumerate(self.layers):\n",
    "            #weight update\n",
    "            if(index==0): #OUTPUT LAYER\n",
    "                err = targets.T - FF_output\n",
    "                err_signal = err * l.d_f(l.net)\n",
    "                l.d_weights = lr * np.dot(l.f(l.input).T, err_signal)\n",
    "            else: #HIDDEN LAYERS\n",
    "                err_signal = np.multiply(l.d_f(l.net), bp.T)\n",
    "                print(\"es\\n\",err_signal)\n",
    "                l.d_weights = lr * np.dot(l.f(l.input).T, err_signal)\n",
    "            l.weights += l.d_weights\n",
    "            \n",
    "            #bias update\n",
    "            l.bias += np.sum(lr*err_signal, axis=0)\n",
    "            \n",
    "            #backprop sum\n",
    "            if(index != len(self.layers)-1):\n",
    "                flat_err_signal = np.prod(err_signal, axis=0)\n",
    "                \n",
    "                if(index==0):\n",
    "                    bp += np.multiply(l.weights, flat_err_signal)\n",
    "                else:\n",
    "                    bp = np.add(np.multiply(l.weights, flat_err_signal), bp)\n",
    "                print(\"bp\\n\", bp)\n",
    "        self.layers.reverse()\n",
    "            \n",
    "        return err\n",
    "    \n",
    "    def train(self,batch, targets, lr):\n",
    "        return self.back_propagation(self.feed_forward(batch), targets, lr)\n",
    "        \n",
    "    def fit(self, dataset, targets, lr, max_it=10000): \n",
    "        for i in range(0, len(nn.layers)):\n",
    "            print(\"old_w: layer \",i)\n",
    "            print(nn.layers[i].weights)\n",
    "            print(nn.layers[i].bias)\n",
    "        \n",
    "        err=0\n",
    "        for i in range(0,max_it):\n",
    "            err=self.train(dataset, targets, lr)\n",
    "            \n",
    "        for i in range(0, len(nn.layers)):\n",
    "            print(\"new_w: layer \",i)\n",
    "            print(nn.layers[i].weights)\n",
    "            print(nn.layers[i].bias)\n",
    "        print(\"err: \\n\", err)\n",
    "    \n",
    "    def predict(self, values):\n",
    "        print(\"prediction: \")\n",
    "        print(self.feed_forward(values))\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "VERSIONE 5\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "    \n",
    "    #Adding input layer(first hidden layer): you must specify the input_size\n",
    "    #Adding hidden/output layer: if the layer is dense, the input_size is inferred from the previous layer\n",
    "    def add_layer(self, l, num_units, f, input_size=None):\n",
    "        if(l == \"dense\" or l == \"Dense\" or l == \"d\" or l == \"D\"):\n",
    "            if(input_size == None):\n",
    "                input_size = self.layers[-1].num_units\n",
    "            \n",
    "            self.layers.append(Layer(num_units, activations(f), input_size))\n",
    "        \n",
    "    def feed_forward(self, batch):\n",
    "        current_input = batch\n",
    "        for l in self.layers:\n",
    "            l.input = current_input\n",
    "            l.net = np.dot(l.input, l.weights) + l.bias # dot(in,w)+bias\n",
    "            l.out = l.f(l.net)\n",
    "            \n",
    "            current_input = l.out\n",
    "        return current_input\n",
    "    \n",
    "    def back_propagation(self, FF_output, targets, lr):\n",
    "        bp=0\n",
    "        err_signal=0\n",
    "        \n",
    "        self.layers.reverse()\n",
    "        for index,l in enumerate(self.layers):\n",
    "            #weight update\n",
    "            if(index==0): #OUTPUT LAYER\n",
    "                err = targets.T - FF_output\n",
    "                err_signal = err * l.d_f(l.net)\n",
    "                l.d_weights = lr * np.dot(l.f(l.input).T, err_signal) \n",
    "            else:\n",
    "                err_signal = np.multiply(l.d_f(l.net), bp.T)\n",
    "                l.d_weights = lr * err_signal * l.f(l.input)\n",
    "            l.weights += l.d_weights\n",
    "            \n",
    "            #bias update\n",
    "            l.bias += np.sum(lr*err_signal, axis=0)\n",
    "            \n",
    "            #backprop sum\n",
    "            flat_err_signal = np.prod(err_signal, axis=0)\n",
    "            bp += flat_err_signal * l.weights\n",
    "        self.layers.reverse()\n",
    "            \n",
    "        return err\n",
    "    \n",
    "    def train(self,batch, targets, lr):\n",
    "        return self.back_propagation(self.feed_forward(batch), targets, lr)\n",
    "        \n",
    "    def fit(self, dataset, targets, lr, max_it=10000): \n",
    "        for i in range(0, len(nn.layers)):\n",
    "            print(\"old_w: layer \",i)\n",
    "            print(nn.layers[i].weights)\n",
    "            print(nn.layers[i].bias)\n",
    "        \n",
    "        err=0\n",
    "        for i in range(0,max_it):\n",
    "            err=self.train(dataset, targets, lr)\n",
    "            \n",
    "        for i in range(0, len(nn.layers)):\n",
    "            print(\"new_w: layer \",i)\n",
    "            print(nn.layers[i].weights)\n",
    "            print(nn.layers[i].bias)\n",
    "        print(\"err: \\n\", err)\n",
    "    \n",
    "    def predict(self, values):\n",
    "        print(\"prediction: \")\n",
    "        print(self.feed_forward(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "VERSIONE 4\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.layers.append(Layer(1, activations('sigmoid'))) #SEMPRE SIGMOID PER L'OUTPUT LAYER(??)\n",
    "    \n",
    "    def add_layer(self,l, num_units, f):\n",
    "        if(l == \"dense\" or l == \"Dense\" or l == \"d\" or l == \"D\"):\n",
    "            self.layers.append(Layer(num_units, activations(f)))\n",
    "        \n",
    "    def feed_forward(self, batch):\n",
    "        current_input = batch\n",
    "        \n",
    "        self.layers.reverse()\n",
    "        for l in self.layers:\n",
    "            l.input = current_input\n",
    "            l.net = np.dot(l.input, l.weights) + l.bias\n",
    "            l.out = l.f(l.net)\n",
    "            \n",
    "            current_input = l.out\n",
    "        self.layers.reverse()\n",
    "        return current_input\n",
    "    \n",
    "    def back_propagation(self, FF_output, targets, lr):\n",
    "        bp=0\n",
    "        \n",
    "        for index,l in enumerate(self.layers):\n",
    "            if(index==0): #OUTPUT LAYER\n",
    "                err = targets.T - FF_output\n",
    "                l.d_weights = lr * np.dot(l.f(l.input).T, err*l.d_f(l.net)) #l.input=batch in this case\n",
    "            else:\n",
    "                l.d_weights = lr * l.d_f(l.net) * bp * l.f(l.input)\n",
    "            l.weights += l.d_weights\n",
    "            bp += l.d_weights * l.weights\n",
    "            \n",
    "        return err\n",
    "    \n",
    "    def train(self,batch, targets, lr):\n",
    "        return self.back_propagation(self.feed_forward(batch), targets, lr)\n",
    "    \n",
    "    def fit(self, dataset, targets, lr, max_it=10000):\n",
    "        #inizializzo i pesi; il numero dipende dal dataset \n",
    "        columns = np.size(dataset,1)\n",
    "        \n",
    "        self.layers.reverse()\n",
    "        prev_output=0\n",
    "        for index,l in enumerate(self.layers):\n",
    "            if(index==0):#PRIMO HIDDEN LAYER, QUELLO CHE PRENDE IN INPUT IL DATASET\n",
    "                l.init_weights(columns)\n",
    "            else:\n",
    "                l.init_weights(prev_output)\n",
    "            prev_output=l.num_units\n",
    "        self.layers.reverse()  \n",
    "        \n",
    "        for i in range(0, len(nn.layers)):\n",
    "            print(\"old_w: layer \",i)\n",
    "            print(nn.layers[i].weights)\n",
    "        \n",
    "        err=0\n",
    "        #for i in range(0,max_it):\n",
    "        #    err=self.train(dataset, targets, lr)\n",
    "            \n",
    "        for i in range(0, len(nn.layers)):\n",
    "            print(\"new_w: layer \",i)\n",
    "            print(nn.layers[i].weights)\n",
    "            \n",
    "        print(\"err: \")\n",
    "        print(err)\n",
    "    \n",
    "    def predict(self, value):\n",
    "        print(\"prediction: \")\n",
    "        print(self.feed_forward(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "VERSIONE 4\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.layers.append(Layer(1, activations('sigmoid'))) #SEMPRE SIGMOID PER L'OUTPUT LAYER(??)\n",
    "    \n",
    "    def add_layer(self,l, num_units, f):\n",
    "        if(l == \"dense\" or l == \"Dense\" or l == \"d\" or l == \"D\"):\n",
    "            self.layers.append(Layer(num_units, activations(f)))\n",
    "        \n",
    "    def feed_forward(self, batch):\n",
    "        current_input = batch\n",
    "        \n",
    "        self.layers.reverse()\n",
    "        for l in self.layers:\n",
    "            l.input = current_input\n",
    "            l.net = np.dot(l.input, l.weights) + l.bias\n",
    "            l.out = l.f(l.net)\n",
    "            \n",
    "            current_input = l.out\n",
    "        self.layers.reverse()\n",
    "        return current_input\n",
    "    \n",
    "    def back_propagation(self, FF_output, targets, lr):\n",
    "        bp=0\n",
    "        \n",
    "        for index,l in enumerate(self.layers):\n",
    "            if(index==0): #OUTPUT LAYER\n",
    "                err = targets.T - FF_output\n",
    "                l.d_weights = lr * np.dot(l.f(l.input).T, err*l.d_f(l.net)) #l.input=batch in this case\n",
    "            else:\n",
    "                l.d_weights = lr * l.d_f(l.net) * bp * l.f(l.input)\n",
    "            l.weights += l.d_weights\n",
    "            bp += l.d_weights * l.weights\n",
    "            \n",
    "        return err\n",
    "    \n",
    "    def train(self,batch, targets, lr):\n",
    "        return self.back_propagation(self.feed_forward(batch), targets, lr)\n",
    "    \n",
    "    def fit(self, dataset, targets, lr, max_it=10000):\n",
    "        #inizializzo i pesi; il numero dipende dal dataset \n",
    "        columns = np.size(dataset,1)\n",
    "        \n",
    "        self.layers.reverse()\n",
    "        prev_output=0\n",
    "        for index,l in enumerate(self.layers):\n",
    "            if(index==0):#PRIMO HIDDEN LAYER, QUELLO CHE PRENDE IN INPUT IL DATASET\n",
    "                l.init_weights(columns)\n",
    "            else:\n",
    "                l.init_weights(prev_output)\n",
    "            prev_output=l.num_units\n",
    "        self.layers.reverse()  \n",
    "        \n",
    "        for i in range(0, len(nn.layers)):\n",
    "            print(\"old_w: layer \",i)\n",
    "            print(nn.layers[i].weights)\n",
    "        \n",
    "        err=0\n",
    "        #for i in range(0,max_it):\n",
    "        #    err=self.train(dataset, targets, lr)\n",
    "            \n",
    "        for i in range(0, len(nn.layers)):\n",
    "            print(\"new_w: layer \",i)\n",
    "            print(nn.layers[i].weights)\n",
    "            \n",
    "        print(\"err: \")\n",
    "        print(err)\n",
    "    \n",
    "    def predict(self, value):\n",
    "        print(\"prediction: \")\n",
    "        print(self.feed_forward(value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
